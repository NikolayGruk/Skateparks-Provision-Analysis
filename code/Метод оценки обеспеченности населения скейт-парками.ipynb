{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install objectnat\n",
    "!pip install pyarrow \n",
    "!pip install IduEdu --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pickle\n",
    "import os\n",
    "import networkx as nx\n",
    "\n",
    "from decimal import *\n",
    "from objectnat import get_service_provision\n",
    "from objectnat import recalculate_links\n",
    "from objectnat import clip_provision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем путь до папки с файлами\n",
    "\n",
    "data_path = 'F:/Резерв/Учеба/НИР-3/Чат gpt/Исходные данные'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем заранее обработанный файл со зданиями\n",
    "\n",
    "buildings = gpd.read_file(os.path.join(data_path, 'buildings.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем, что файл со зданиями не содержит ошибок\n",
    "\n",
    "buildings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем спрос для каждого диапазона времени. Коэффициенты 0.43 и 0.38 взяты из результатов социологического опроса\n",
    "\n",
    "buildings['demand_30'] = (buildings['demand'] * 0.43).round().astype(int)\n",
    "buildings['demand_60'] = (buildings['demand'] * 0.38).round().astype(int)\n",
    "buildings['demand_90'] = buildings['demand'] - buildings['demand_30'] - buildings['demand_60']\n",
    "buildings['total_demand'] = buildings['demand_30'] + buildings['demand_60'] + buildings['demand_90']\n",
    "\n",
    "buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем заранее обработанный файл со скейт-парками\n",
    "\n",
    "skateparks = gpd.read_file(os.path.join(data_path, 'skateparks.geojson'))\n",
    "\n",
    "# Смотрим как он выглядит и проверяем, что все атрибуты на месте\n",
    "\n",
    "skateparks.explore(column='capacity',tiles='CartoDB positron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисление суммарного спроса по категориям\n",
    "\n",
    "total_population = buildings['demand'].sum()\n",
    "print(f\"Суммарная численность населения: {total_population}\")\n",
    "\n",
    "demand_30 = buildings['demand_30'].sum()\n",
    "print(f\"Суммарный спрос в границах 30 минут: {demand_30}\")\n",
    "\n",
    "demand_60 = buildings['demand_60'].sum()\n",
    "print(f\"Суммарный спрос в границах 60 минут: {demand_60}\")\n",
    "\n",
    "demand_90 = buildings['demand_90'].sum()\n",
    "print(f\"Суммарный спрос в границах 90 минут: {demand_90}\")\n",
    "\n",
    "total_demand = buildings['total_demand'].sum()\n",
    "print(f\"Общий суммарный спрос: {total_demand}\")\n",
    "\n",
    "total_capacity = skateparks['capacity'].sum()\n",
    "print(\"Суммарное количество людей, которые может обслужить скейт-парк:\", total_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка матриц смежности\n",
    "\n",
    "matrix = pd.read_csv('F:/Резерв/Учеба/НИР-3/Чат gpt/matrix_time.csv', index_col=0)\n",
    "matrix_30 = pd.read_csv('F:/Резерв/Учеба/НИР-3/Чат gpt/matrix_30.csv', index_col=0)\n",
    "matrix_60 = pd.read_csv('F:/Резерв/Учеба/НИР-3/Чат gpt/matrix_60.csv', index_col=0)\n",
    "matrix_90 = pd.read_csv('F:/Резерв/Учеба/НИР-3/Чат gpt/matrix_90.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapacityKeyError(KeyError):\n",
    "    def __init__(self, *args):\n",
    "        if args:\n",
    "            self.message = args[0]\n",
    "        else:\n",
    "            self.message = None\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.message:\n",
    "            return \"CapacityKeyError, {0} \".format(self.message)\n",
    "\n",
    "        return (\n",
    "            \"Column 'capacity' was not found in provided 'services' GeoDataFrame. This attribute \"\n",
    "            \"corresponds to the total capacity for each service.\"\n",
    "        )\n",
    "\n",
    "\n",
    "class CapacityValueError(ValueError):\n",
    "    def __init__(self, *args):\n",
    "        if args:\n",
    "            self.message = args[0]\n",
    "        else:\n",
    "            self.message = None\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.message:\n",
    "            return \"CapacityValueError, {0} \".format(self.message)\n",
    "\n",
    "        return \"Column 'capacity' in 'services' GeoDataFrame  has no valid value.\"\n",
    "\n",
    "\n",
    "class DemandKeyError(KeyError):\n",
    "    def __init__(self, *args):\n",
    "        if args:\n",
    "            self.message = args[0]\n",
    "        else:\n",
    "            self.message = None\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.message:\n",
    "            return \"DemandKeyError, {0} \".format(self.message)\n",
    "\n",
    "        return (\n",
    "            \"The column 'demand' was not found in the provided 'demanded_buildings' GeoDataFrame. \"\n",
    "            \"This attribute corresponds to the number of demands for the selected service in each building.\"\n",
    "        )\n",
    "\n",
    "\n",
    "class DemandValueError(ValueError):\n",
    "    def __init__(self, *args):\n",
    "        if args:\n",
    "            self.message = args[0]\n",
    "        else:\n",
    "            self.message = None\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.message:\n",
    "            return \"DemandValueError, {0} \".format(self.message)\n",
    "        return \"Column 'demand' in 'demanded_buildings' GeoDataFrame  has no valid value.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=singleton-comparison\n",
    "from typing import Tuple\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from shapely import LineString\n",
    "\n",
    "from objectnat import config\n",
    "\n",
    "from provision_exceptions import CapacityKeyError, DemandKeyError\n",
    "\n",
    "logger = config.logger\n",
    "\n",
    "\n",
    "class Provision:\n",
    "    \"\"\"\n",
    "    Represents the logic for city provision calculations using a gravity or linear model.\n",
    "\n",
    "    Args:\n",
    "        services (InstanceOf[gpd.GeoDataFrame]): GeoDataFrame representing the services available in the city.\n",
    "        demanded_buildings (InstanceOf[gpd.GeoDataFrame]): GeoDataFrame representing the buildings with demands for services.\n",
    "        adjacency_matrix (InstanceOf[pd.DataFrame]): DataFrame representing the adjacency matrix between buildings.\n",
    "        threshold (int): Threshold value for the provision calculations.\n",
    "        calculation_type (str, optional): Type of calculation (\"gravity\" or \"linear\"). Defaults to \"gravity\".\n",
    "\n",
    "    Returns:\n",
    "        Provision: The CityProvision object.\n",
    "\n",
    "    Raises: KeyError: If the 'demand' column is missing in the provided 'demanded_buildings' GeoDataFrame,\n",
    "    or if the 'capacity' column is missing in the provided 'services' GeoDataFrame. ValueError: If the 'capacity'\n",
    "    column in 'services' or 'demand' column  'demanded_buildings' GeoDataFrame has no valid value.\n",
    "    \"\"\"\n",
    "\n",
    "    destination_matrix = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        services: gpd.GeoDataFrame,\n",
    "        demanded_buildings: gpd.GeoDataFrame,\n",
    "        adjacency_matrix: pd.DataFrame,\n",
    "        threshold: int,\n",
    "    ):\n",
    "        self.services = self.ensure_services(services.copy())\n",
    "        self.demanded_buildings = self.ensure_buildings(demanded_buildings.copy())\n",
    "        self.adjacency_matrix = self.delete_useless_matrix_rows_columns(\n",
    "            adjacency_matrix.copy(), demanded_buildings, services\n",
    "        ).copy()\n",
    "        self.threshold = threshold\n",
    "        self.check_crs(self.demanded_buildings, self.services)\n",
    "        pandarallel.initialize(progress_bar=False, verbose=0, use_memory_fs=config.pandarallel_use_file_system)\n",
    "\n",
    "    @staticmethod\n",
    "    def ensure_buildings(v: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "        if \"demand\" not in v.columns:\n",
    "            raise DemandKeyError\n",
    "        v[\"demand_left\"] = v[\"demand\"]\n",
    "        return v\n",
    "\n",
    "    @staticmethod\n",
    "    def ensure_services(v: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "        if \"capacity\" not in v.columns:\n",
    "            raise CapacityKeyError\n",
    "        v[\"capacity_left\"] = v[\"capacity\"]\n",
    "        return v\n",
    "\n",
    "    @staticmethod\n",
    "    def check_crs(demanded_buildings, services):\n",
    "        assert (\n",
    "            demanded_buildings.crs == services.crs\n",
    "        ), f\"\\nThe CRS in the provided geodataframes are different.\\nBuildings CRS:{demanded_buildings.crs}\\nServices CRS:{services.crs} \\n\"\n",
    "\n",
    "    @staticmethod\n",
    "    def delete_useless_matrix_rows_columns(adjacency_matrix, demanded_buildings, services):\n",
    "        adjacency_matrix.index = adjacency_matrix.index.astype(int)\n",
    "\n",
    "        builds_indexes = set(demanded_buildings.index.astype(int).tolist())\n",
    "        rows = set(adjacency_matrix.index.astype(int).tolist())\n",
    "        dif = rows ^ builds_indexes\n",
    "        adjacency_matrix.drop(index=(list(dif)), axis=0, inplace=True)\n",
    "\n",
    "        service_indexes = set(services.index.astype(int).tolist())\n",
    "        columns = set(adjacency_matrix.columns.astype(int).tolist())\n",
    "        dif = columns ^ service_indexes\n",
    "        adjacency_matrix.drop(columns=(list(dif)), axis=0, inplace=True)\n",
    "        return adjacency_matrix.transpose()\n",
    "\n",
    "    def run(self) -> Tuple[gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame]:\n",
    "\n",
    "        def apply_function_based_on_size(df, func, axis, threshold=100):\n",
    "            if len(df) > threshold:\n",
    "                return df.parallel_apply(func, axis=axis)\n",
    "            return df.apply(func, axis=axis)\n",
    "\n",
    "        def calculate_flows_y(loc):\n",
    "            import numpy as np  # pylint: disable=redefined-outer-name,reimported,import-outside-toplevel\n",
    "            import pandas as pd  # pylint: disable=redefined-outer-name,reimported,import-outside-toplevel\n",
    "\n",
    "            c = services_table.loc[loc.name][\"capacity_left\"]\n",
    "            a = services_table.loc[loc.name][\"Rating\"]\n",
    "            p = a / loc / loc\n",
    "            p = p / p.sum()\n",
    "            threshold = p.quantile(best_houses)\n",
    "            p = p[p >= threshold]\n",
    "            p = p / p.sum()\n",
    "            if p.sum() == 0:\n",
    "                return loc\n",
    "            rng = np.random.default_rng(seed=0)\n",
    "            r = pd.Series(0, p.index)\n",
    "            choice = np.unique(rng.choice(p.index, int(c), p=p.values), return_counts=True)\n",
    "            choice = r.add(pd.Series(choice[1], choice[0]), fill_value=0)\n",
    "\n",
    "            return choice\n",
    "\n",
    "        def balance_flows_to_demands(loc):\n",
    "            import numpy as np  # pylint: disable=redefined-outer-name,reimported,import-outside-toplevel\n",
    "            import pandas as pd  # pylint: disable=redefined-outer-name,reimported,import-outside-toplevel\n",
    "\n",
    "            d = houses_table.loc[loc.name][\"demand_left\"]\n",
    "            loc = loc[loc > 0]\n",
    "            if loc.sum() > 0:\n",
    "                p = loc / loc.sum()\n",
    "                rng = np.random.default_rng(seed=0)\n",
    "                r = pd.Series(0, p.index)\n",
    "                choice = np.unique(rng.choice(p.index, int(d), p=p.values), return_counts=True)\n",
    "                choice = r.add(pd.Series(choice[1], choice[0]), fill_value=0)\n",
    "                choice = pd.Series(\n",
    "                    data=np.minimum(loc.sort_index().values, choice.sort_index().values),\n",
    "                    index=loc.sort_index().index,\n",
    "                )\n",
    "                return choice\n",
    "            return loc\n",
    "\n",
    "        logger.debug(\n",
    "            f\"Calculating provision from {len(self.services)} services to {len(self.demanded_buildings)} buildings.\"\n",
    "        )\n",
    "\n",
    "        distance_matrix = self.adjacency_matrix\n",
    "        destination_matrix = pd.DataFrame(\n",
    "            0,\n",
    "            index=distance_matrix.index,\n",
    "            columns=distance_matrix.columns,\n",
    "            dtype=int,\n",
    "        )\n",
    "        distance_matrix = distance_matrix.where(distance_matrix <= self.threshold * 3, np.inf)\n",
    "\n",
    "        houses_table = self.demanded_buildings[[\"demand\", \"demand_left\"]].copy()\n",
    "        services_table = self.services[[\"capacity\", \"capacity_left\"]].copy()\n",
    "        distance_matrix = distance_matrix.drop(\n",
    "            index=services_table[services_table[\"capacity_left\"] == 0].index.values,\n",
    "            columns=houses_table[houses_table[\"demand_left\"] == 0].index.values,\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "        distance_matrix = distance_matrix.loc[~(distance_matrix == np.inf).all(axis=1)]\n",
    "        distance_matrix = distance_matrix.loc[:, ~(distance_matrix == np.inf).all(axis=0)]\n",
    "\n",
    "        distance_matrix = distance_matrix + 1\n",
    "        selection_range = (self.threshold + 1) / 2\n",
    "        best_houses = 0.9\n",
    "        while len(distance_matrix.columns) > 0 and len(distance_matrix.index) > 0:\n",
    "            objects_n = sum(distance_matrix.shape)\n",
    "            logger.debug(\n",
    "                f\"Matrix shape: {distance_matrix.shape},\"\n",
    "                f\" Total objects: {objects_n},\"\n",
    "                f\" Selection range: {selection_range},\"\n",
    "                f\" Best houses: {best_houses}\"\n",
    "            )\n",
    "\n",
    "            temp_destination_matrix = apply_function_based_on_size(\n",
    "                distance_matrix, lambda x: calculate_flows_y(x[x <= selection_range]), 1\n",
    "            )\n",
    "\n",
    "            temp_destination_matrix = temp_destination_matrix.fillna(0)\n",
    "            temp_destination_matrix = apply_function_based_on_size(temp_destination_matrix, balance_flows_to_demands, 0)\n",
    "            temp_destination_matrix = temp_destination_matrix.fillna(0)\n",
    "            temp_destination_matrix_aligned = temp_destination_matrix.reindex(\n",
    "                index=destination_matrix.index, columns=destination_matrix.columns, fill_value=0\n",
    "            )\n",
    "            del temp_destination_matrix\n",
    "            destination_matrix_np = destination_matrix.to_numpy()\n",
    "            temp_destination_matrix_np = temp_destination_matrix_aligned.to_numpy()\n",
    "            del temp_destination_matrix_aligned\n",
    "            destination_matrix = pd.DataFrame(\n",
    "                destination_matrix_np + temp_destination_matrix_np,\n",
    "                index=destination_matrix.index,\n",
    "                columns=destination_matrix.columns,\n",
    "            )\n",
    "            del destination_matrix_np, temp_destination_matrix_np\n",
    "            axis_1 = destination_matrix.sum(axis=1).astype(int)\n",
    "            axis_0 = destination_matrix.sum(axis=0).astype(int)\n",
    "\n",
    "            services_table[\"capacity_left\"] = services_table[\"capacity\"].subtract(axis_1, fill_value=0)\n",
    "            houses_table[\"demand_left\"] = houses_table[\"demand\"].subtract(axis_0, fill_value=0)\n",
    "            del axis_1, axis_0\n",
    "            distance_matrix = distance_matrix.drop(\n",
    "                index=services_table[services_table[\"capacity_left\"] == 0].index.values,\n",
    "                columns=houses_table[houses_table[\"demand_left\"] == 0].index.values,\n",
    "                errors=\"ignore\",\n",
    "            )\n",
    "            distance_matrix = distance_matrix.loc[~(distance_matrix == np.inf).all(axis=1)]\n",
    "            distance_matrix = distance_matrix.loc[:, ~(distance_matrix == np.inf).all(axis=0)]\n",
    "\n",
    "            selection_range *= 1.5\n",
    "            if best_houses <= 0.1:\n",
    "                best_houses = 0\n",
    "            else:\n",
    "                objects_n_new = sum(distance_matrix.shape)\n",
    "                best_houses = objects_n_new / (objects_n / best_houses)\n",
    "\n",
    "        logger.debug(\"Done!\")\n",
    "        del distance_matrix, houses_table, services_table\n",
    "        self.destination_matrix = destination_matrix\n",
    "\n",
    "        _additional_options(\n",
    "            self.demanded_buildings,\n",
    "            self.services,\n",
    "            self.adjacency_matrix,\n",
    "            self.destination_matrix,\n",
    "            self.threshold,\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            self.demanded_buildings,\n",
    "            self.services,\n",
    "            _calc_links(\n",
    "                self.destination_matrix,\n",
    "                self.services,\n",
    "                self.demanded_buildings,\n",
    "                self.adjacency_matrix,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "def _calc_links(\n",
    "    destination_matrix: pd.DataFrame,\n",
    "    services: gpd.GeoDataFrame,\n",
    "    buildings: gpd.GeoDataFrame,\n",
    "    distance_matrix: pd.DataFrame,\n",
    "):\n",
    "    buildings_ = buildings.copy()\n",
    "    services_ = services.copy()\n",
    "    buildings_.geometry = buildings_.representative_point()\n",
    "    services_.geometry = services_.representative_point()\n",
    "\n",
    "    def subfunc(loc):\n",
    "        try:\n",
    "            return [\n",
    "                {\n",
    "                    \"building_index\": int(k),\n",
    "                    \"demand\": int(v),\n",
    "                    \"service_index\": int(loc.name),\n",
    "                }\n",
    "                for k, v in loc.to_dict().items()\n",
    "            ]\n",
    "        except:\n",
    "            return np.NaN\n",
    "\n",
    "    def subfunc_geom(loc):\n",
    "        return LineString(\n",
    "            (\n",
    "                buildings_.geometry[loc[\"building_index\"]],\n",
    "                services_.geometry[loc[\"service_index\"]],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    flat_matrix = destination_matrix.transpose().apply(lambda x: subfunc(x[x > 0]), result_type=\"reduce\")\n",
    "\n",
    "    distribution_links = gpd.GeoDataFrame(data=[item for sublist in list(flat_matrix) for item in sublist])\n",
    "\n",
    "    distribution_links[\"distance\"] = distribution_links.apply(\n",
    "        lambda x: distance_matrix.loc[x[\"service_index\"]][x[\"building_index\"]],\n",
    "        axis=1,\n",
    "        result_type=\"reduce\",\n",
    "    )\n",
    "\n",
    "    sel = distribution_links[\"building_index\"].isin(buildings_.index.values) & distribution_links[\"service_index\"].isin(\n",
    "        services_.index.values\n",
    "    )\n",
    "    sel = distribution_links.loc[sel[sel].index.values]\n",
    "    distribution_links = distribution_links.set_geometry(sel.apply(subfunc_geom, axis=1)).set_crs(buildings_.crs)\n",
    "    distribution_links[\"distance\"] = distribution_links[\"distance\"].astype(float).round(2)\n",
    "    return distribution_links\n",
    "\n",
    "\n",
    "def _additional_options(\n",
    "    buildings,\n",
    "    services,\n",
    "    matrix,\n",
    "    destination_matrix,\n",
    "    normative_distance,\n",
    "):\n",
    "    buildings[\"avg_dist\"] = 0\n",
    "    buildings[\"supplyed_demands_within\"] = 0\n",
    "    buildings[\"supplyed_demands_without\"] = 0\n",
    "    services[\"carried_capacity_within\"] = 0\n",
    "    services[\"carried_capacity_without\"] = 0\n",
    "    for i, loc in destination_matrix.iterrows():\n",
    "        distances_all = matrix.loc[loc.name]\n",
    "        distances = distances_all[distances_all <= normative_distance]\n",
    "        s = matrix.loc[loc.name] <= normative_distance\n",
    "        within = loc[s]\n",
    "        without = loc[~s]\n",
    "        within = within[within > 0]\n",
    "        without = without[without > 0]\n",
    "        buildings[\"avg_dist\"] = (\n",
    "            buildings[\"avg_dist\"]\n",
    "            .add(distances.multiply(within, fill_value=0), fill_value=0)\n",
    "            .add(distances_all.multiply(without, fill_value=0), fill_value=0)\n",
    "        )\n",
    "        buildings[\"demand_left\"] = buildings[\"demand_left\"].sub(within.add(without, fill_value=0), fill_value=0)\n",
    "        buildings[\"supplyed_demands_within\"] = buildings[\"supplyed_demands_within\"].add(within, fill_value=0)\n",
    "        buildings[\"supplyed_demands_without\"] = buildings[\"supplyed_demands_without\"].add(without, fill_value=0)\n",
    "\n",
    "        services.at[loc.name, \"capacity_left\"] = (\n",
    "            services.at[loc.name, \"capacity_left\"] - within.add(without, fill_value=0).sum()\n",
    "        )\n",
    "        services.at[loc.name, \"carried_capacity_within\"] = (\n",
    "            services.at[loc.name, \"carried_capacity_within\"] + within.sum()\n",
    "        )\n",
    "        services.at[loc.name, \"carried_capacity_without\"] = (\n",
    "            services.at[loc.name, \"carried_capacity_without\"] + without.sum()\n",
    "        )\n",
    "    buildings[\"min_dist\"] = matrix.min(axis=0).replace(np.inf, None)\n",
    "    buildings[\"avg_dist\"] = (buildings[\"avg_dist\"] / (buildings[\"demand\"] - buildings[\"demand_left\"])).astype(\n",
    "        np.float32\n",
    "    )\n",
    "    buildings[\"avg_dist\"] = buildings.apply(\n",
    "        lambda x: np.nan if (x[\"demand\"] == x[\"demand_left\"]) else round(x[\"avg_dist\"], 2), axis=1\n",
    "    )\n",
    "    buildings[\"provison_value\"] = (buildings[\"supplyed_demands_within\"] / buildings[\"demand\"]).astype(float).round(2)\n",
    "    services[\"service_load\"] = (services[\"capacity\"] - services[\"capacity_left\"]).astype(np.uint16)\n",
    "    buildings[\"supplyed_demands_within\"] = buildings[\"supplyed_demands_within\"].astype(np.uint16)\n",
    "    buildings[\"supplyed_demands_without\"] = buildings[\"supplyed_demands_without\"].astype(np.uint16)\n",
    "    services[\"carried_capacity_within\"] = services[\"carried_capacity_within\"].astype(np.uint16)\n",
    "    services[\"carried_capacity_without\"] = services[\"carried_capacity_without\"].astype(np.uint16)\n",
    "    logger.debug(\"Done adding additional options\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_service_provision(\n",
    "    buildings: gpd.GeoDataFrame,\n",
    "    adjacency_matrix: pd.DataFrame,\n",
    "    services: gpd.GeoDataFrame,\n",
    "    threshold: int,\n",
    "    buildings_demand_column: str = \"demand\",\n",
    "    services_capacity_column: str = \"capacity\",\n",
    "    services_rating_column: str = \"Rating\",\n",
    ") -> Tuple[gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame]:\n",
    "    \"\"\"Calculate load from buildings with demands on the given services using the distances matrix between them.\n",
    "\n",
    "    Args:\n",
    "        services (gpd.GeoDataFrame): GeoDataFrame of services\n",
    "        adjacency_matrix (pd.DataFrame): DataFrame representing the adjacency matrix\n",
    "        buildings (gpd.GeoDataFrame): GeoDataFrame of demanded buildings\n",
    "        threshold (int): Threshold value\n",
    "        buildings_demand_column (str): column name of buildings demands\n",
    "        services_capacity_column (str): column name of services capacity\n",
    "    Returns:\n",
    "        Tuple[gpd.GeoDataFrame, gpd.GeoDataFrame, gpd.GeoDataFrame]: Tuple of GeoDataFrames representing provision\n",
    "        buildings, provision services, and provision links\n",
    "    \"\"\"\n",
    "    buildings = buildings.copy()\n",
    "    services = services.copy()\n",
    "    adjacency_matrix = adjacency_matrix.copy()\n",
    "    buildings[\"demand\"] = buildings[buildings_demand_column]\n",
    "    services[\"capacity\"] = services[services_capacity_column]\n",
    "    services[\"Rating\"] = services[services_rating_column]\n",
    "\n",
    "\n",
    "    provision_buildings, provision_services, provision_links = Provision(\n",
    "        services=services,\n",
    "        demanded_buildings=buildings,\n",
    "        adjacency_matrix=adjacency_matrix,\n",
    "        threshold=threshold,\n",
    "    ).run()\n",
    "    return provision_buildings, provision_services, provision_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расчет оценки обеспеченности\n",
    "\n",
    "from objectnat import get_service_provision\n",
    "\n",
    "buildings['demand'] = buildings['demand_30'] # для сценария 1 указывается 'demand_30', для сценария 2 'demand_60', для сценария 3 'demand_90'\n",
    "#skateparks['capacity'] = buildings['total_demand'].sum() # для экспериментального сценария\n",
    "builds = buildings.to_crs(32636)\n",
    "services = skateparks.to_crs(32636)\n",
    "adjacency_matrix = matrix_30 # для сценария 1 указывается matrix_30, для сценария 2 matrix_60, для сценария 3 matrix_90\n",
    "# Reading building and service data, reprojecting them to EPSG:32636 (UTM zone), and loading the precomputed adjacency matrix\n",
    "\n",
    "adjacency_matrix.index = adjacency_matrix.index.astype(int)\n",
    "services.index = services.index.astype(int)\n",
    "builds.index = builds.index.astype(int)\n",
    "adjacency_matrix.columns = adjacency_matrix.columns.astype(int)\n",
    "# Ensuring the indices and columns of the adjacency matrix and GeoDataFrames are integers for proper matching\n",
    "\n",
    "build_prov, services_prov, links_prov = get_service_provision(\n",
    "    buildings=builds,\n",
    "    services=services,\n",
    "    adjacency_matrix=adjacency_matrix,\n",
    "    threshold=30, # для сценария 1 threshold=30, для сценария 2 threshold=60, для сценария 3 threshold=90\n",
    ")\n",
    "# Calculating service provision for each building. The 'threshold' parameter (10) defines the maximum allowable distance (or time)\n",
    "# beyond which buildings are considered to not meet the required service provision.\n",
    "\n",
    "services_prov['load'] = services_prov['service_load'] / services_prov['capacity']\n",
    "\n",
    "# Скачивание файлов для основных сценариев\n",
    "build_prov.to_file(os.path.join(data_path,\"build_prov_30.geojson\"))\n",
    "services_prov.to_file(os.path.join(data_path,\"services_prov_30.geojson\"))\n",
    "links_prov.to_file(os.path.join(data_path,\"links_prov_30.geojson\"))\n",
    "\n",
    "# Скачивание файлов для экспериментальных сценариев\n",
    "#build_prov.to_file(os.path.join(data_path,\"build_prov_exp_30.geojson\"))\n",
    "#services_prov.to_file(os.path.join(data_path,\"services_prov_exp_30.geojson\"))\n",
    "#links_prov.to_file(os.path.join(data_path,\"links_prov_exp_30.geojson\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация полученных данных\n",
    "\n",
    "# Visualize data\n",
    "m1 = build_prov.reset_index().explore(column='avg_dist', cmap='RdYlGn_r', tiles='CartoDB positron')\n",
    "# Visualizing buildings provision on a map using 'explore'. \n",
    "# The 'avg_dist' column (average distance to services) is visualized with a color map 'RdYlGn_r'\n",
    "\n",
    "links_prov.explore(m=m1, column='service_index', cmap='prism', style_kwds={'opacity': 0.5})\n",
    "# Visualizing links (connections between buildings and services) on the same map.\n",
    "# 'service_index' shows which service each link corresponds to.\n",
    "\n",
    "services_prov.explore(m=m1, color='red')\n",
    "# Visualizing service locations on the same map, using red markers for easy distinction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение полученных данных в файл\n",
    "\n",
    "m1.save('Основной сценарий - 30 минут.html')\n",
    "# m1.save('Экспериментальный сценарий - 30 минут.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будет происходить агрегация полученых результатов по районам и подсчет итогового показателя обеспеченности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем нужный файл с посчитанной обеспеченностью для жилых домов и скейт-парков\n",
    "\n",
    "build_prov = gpd.read_file(os.path.join(data_path, 'build_prov_30.geojson'))\n",
    "services_prov = gpd.read_file(os.path.join(data_path, 'services_prov_30.geojson'))\n",
    "\n",
    "# build_prov = gpd.read_file(os.path.join(data_path, 'build_prov_exp_90.geojson'))\n",
    "# services_prov = gpd.read_file(os.path.join(data_path, 'services_prov_exp_90.geojson'))\n",
    "\n",
    "# Загружаем слои с границами административных районов и муниципальных образований\n",
    "\n",
    "administrative_blocks = gpd.read_file(os.path.join(data_path, 'administrative_blocks.geojson'))\n",
    "munitipal_blocks = gpd.read_file(os.path.join(data_path, 'munitipal_blocks.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Агрегация данных по зданиям по административным районам\n",
    "\n",
    "aggregated = build_prov.groupby('Administrative').agg(\n",
    "    total_demand=('total_demand', 'sum'),   # Вычисление суммарного спроса\n",
    "    supplyed_demands_within=('supplyed_demands_within', 'sum'), # Вычисление суммарного спроса, удовлетворенного в границах доступности\n",
    "    supplyed_demands_without=('supplyed_demands_without', 'sum'),   # Вычисление суммарного спроса, удовлетворенного вне границ доступности\n",
    "    total_left_demand=('demand_left', 'sum'),    # Вычисление суммарного спроса, оставшегося неудовлетворенным\n",
    "    mean_dist=('avg_dist','mean'),  # Вычисление среднего расстояния до скейт-парка\n",
    "    mean_prov=('provison_value','mean')    # Вычисление среднего значения обеспеченности\n",
    ").reset_index()\n",
    "\n",
    "aggregated['total_left_demand'] = aggregated['total_left_demand'].astype('int32')\n",
    "aggregated['mean_dist'] = aggregated['mean_dist'].round(2)\n",
    "aggregated['mean_prov'] = aggregated['mean_prov'].round(3)\n",
    "aggregated['level_of_demand'] = round(((aggregated['total_demand'] - aggregated['total_left_demand']) / aggregated['total_demand']),3)\n",
    "\n",
    "# Агрегация данных по скейт-паркам по административным районам\n",
    "\n",
    "skateparks_aggregated = services_prov.groupby('Administrative').agg(\n",
    "    skateparks_count=('TableID','count'), # Вычисление общего количества скейт-парков\n",
    "    sum_of_capacity=('capacity', 'sum'), # Вычисление суммарной вместимости скейт-парков\n",
    "    carried_capacity_within=('carried_capacity_within', 'sum'), # Вычисление суммарной вместимости, используемой в границах доступности\n",
    "    carried_capacity_without=('carried_capacity_without', 'sum'), # Вычисление суммарной вместимости, используемой вне границ доступности\n",
    "    capacity_left=('capacity_left', 'sum'), # Вычисление суммарной вестимости оставшейся неиспользуемой вместимости\n",
    "    service_load=('service_load', 'mean'), # Вычисление среднего количества посетителей скейт-парка\n",
    "    mean_load=('load', 'mean') # Вычисление среднего уровня загруженности скейт-парка\n",
    ").reset_index()\n",
    "\n",
    "skateparks_aggregated['skateparks_count'] = skateparks_aggregated['skateparks_count'].astype('int32')\n",
    "skateparks_aggregated['service_load'] = skateparks_aggregated['service_load'].round(0).astype('int32')\n",
    "skateparks_aggregated['mean_load'] = skateparks_aggregated['mean_load'].round(3)\n",
    "\n",
    "# Объединение данных по домам и скейт-паркам\n",
    "\n",
    "merged_df = pd.merge(aggregated, skateparks_aggregated, on='Administrative', how='left')\n",
    "\n",
    "# Удаление NaN-значений и приведение к нужному типу данных\n",
    "merged_df['skateparks_count'] = merged_df['skateparks_count'].fillna(0).astype('int32')\n",
    "merged_df['sum_of_capacity'] = merged_df['sum_of_capacity'].fillna(0).astype('int32')\n",
    "merged_df['carried_capacity_within'] = merged_df['carried_capacity_within'].fillna(0).astype('int32')\n",
    "merged_df['carried_capacity_without'] = merged_df['carried_capacity_without'].fillna(0).astype('int32')\n",
    "merged_df['capacity_left'] = merged_df['capacity_left'].fillna(0).astype('int32')\n",
    "merged_df['service_load'] = merged_df['service_load'].fillna(0).astype('int32')\n",
    "merged_df['mean_load'] = merged_df['mean_load'].fillna(0)\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение функции для линейной нормализации\n",
    "def min_max_normalization(column):\n",
    "    return (column - column.min()) / (column.max() - column.min())\n",
    "\n",
    "# Определение функции для обратной нормализации\n",
    "def inverse_min_max_normalization(column):\n",
    "    return 1 - (column - column.min()) / (column.max() - column.min())\n",
    "\n",
    "# Определяем суммарный уровень загруженности скейт-парков\n",
    "merged_df['skateparks_load'] = merged_df['skateparks_count'] * merged_df['mean_load']\n",
    "\n",
    "# Применение функции к нужным столбцам\n",
    "normalized_layer = merged_df.copy()  # Создаем копию DataFrame для нормализации\n",
    "normalized_layer[['norm_mean_provision', 'norm_skateparks_load_level']] = normalized_layer[['mean_prov', 'skateparks_load']].apply(min_max_normalization)\n",
    "normalized_layer[['norm_mean_dist']] = normalized_layer[['mean_dist']].apply(inverse_min_max_normalization)\n",
    "\n",
    "# Расчет итоговой оценки\n",
    "normalized_layer['final_assessment'] = normalized_layer['norm_mean_dist'] + normalized_layer['norm_mean_provision'] + normalized_layer['norm_skateparks_load_level']\n",
    "\n",
    "# Округление полученных значений\n",
    "normalized_layer[['norm_mean_provision', 'norm_skateparks_load_level', 'norm_mean_dist', 'final_assessment']] = normalized_layer[['norm_mean_provision', 'norm_skateparks_load_level', 'norm_mean_dist', 'final_assessment']].round(3)\n",
    "\n",
    "# Визуализация данных\n",
    "normalized_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем агрегированные данные с геометрией административных районов и сохраняем в файл\n",
    "merged_gdf = administrative_blocks.merge(normalized_layer, on='Administrative')\n",
    "merged_gdf.to_file(os.path.join(data_path,\"Основной сценарий 30 минут - Административные районы.geojson\"))\n",
    "# merged_gdf.to_file(os.path.join(data_path,\"Экспериментальный сценарий 30 минут - Административные районы.geojson\"))\n",
    "\n",
    "# Объединяем агрегированные данные с геометрией административных районов и сохраняем в файл\n",
    "map = merged_gdf.explore(column='final_assessment', cmap='RdYlGn', tiles='CartoDB positron')\n",
    "map.save('Основной сценарий 30 минут - Административные районы.html')\n",
    "# map.save('Экспериментальный сценарий 30 минут - Административные районы.html')\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повторяем все тоже самое, но уже не для административных районов, а для муниципальных образований"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Агрегация данных по зданиям по муниципальным образованиям\n",
    "\n",
    "mun_aggregated = build_prov.groupby('Munitipal').agg(\n",
    "    total_demand=('total_demand', 'sum'),   # Вычисление суммарного спроса\n",
    "    supplyed_demands_within=('supplyed_demands_within', 'sum'), # Вычисление суммарного спроса, удовлетворенного в границах доступности\n",
    "    supplyed_demands_without=('supplyed_demands_without', 'sum'),   # Вычисление суммарного спроса, удовлетворенного вне границ доступности\n",
    "    total_left_demand=('demand_left', 'sum'),    # Вычисление суммарного спроса, оставшегося неудовлетворенным\n",
    "    mean_dist=('avg_dist','mean'),  # Вычисление среднего расстояния до скейт-парка\n",
    "    mean_prov=('provison_value','mean')    # Вычисление среднего значения обеспеченности\n",
    ").reset_index()\n",
    "\n",
    "mun_aggregated['total_left_demand'] = mun_aggregated['total_left_demand'].astype('int32')\n",
    "mun_aggregated['mean_dist'] = mun_aggregated['mean_dist'].round(2).fillna(0)\n",
    "mun_aggregated['mean_prov'] = mun_aggregated['mean_prov'].round(3).fillna(0)\n",
    "mun_aggregated['level_of_demand'] = round(((mun_aggregated['total_demand'] - mun_aggregated['total_left_demand']) / mun_aggregated['total_demand']),3).fillna(0)\n",
    "\n",
    "# Агрегация данных по скейт-паркам по административным районам\n",
    "\n",
    "skateparks_aggregated = services_prov.groupby('Munitipal').agg(\n",
    "    skateparks_count=('TableID','count'), # Вычисление общего количества скейт-парков\n",
    "    sum_of_capacity=('capacity', 'sum'), # Вычисление суммарной вместимости скейт-парков\n",
    "    carried_capacity_within=('carried_capacity_within', 'sum'), # Вычисление суммарной вместимости, используемой в границах доступности\n",
    "    carried_capacity_without=('carried_capacity_without', 'sum'), # Вычисление суммарной вместимости, используемой вне границ доступности\n",
    "    capacity_left=('capacity_left', 'sum'), # Вычисление суммарной вестимости оставшейся неиспользуемой вместимости\n",
    "    service_load=('service_load', 'mean'), # Вычисление среднего количества посетителей скейт-парка\n",
    "    mean_load=('load', 'mean') # Вычисление среднего уровня загруженности скейт-парка\n",
    ").reset_index()\n",
    "\n",
    "skateparks_aggregated['skateparks_count'] = skateparks_aggregated['skateparks_count'].astype('int32')\n",
    "skateparks_aggregated['service_load'] = skateparks_aggregated['service_load'].round(0).astype('int32')\n",
    "skateparks_aggregated['mean_load'] = skateparks_aggregated['mean_load'].round(3)\n",
    "\n",
    "# Объединение данных по домам и скейт-паркам\n",
    "\n",
    "mun_merged_df = pd.merge(mun_aggregated, skateparks_aggregated, on='Munitipal', how='left')\n",
    "\n",
    "# Удаление NaN-значений и приведение к нужному типу данных\n",
    "mun_merged_df['skateparks_count'] = mun_merged_df['skateparks_count'].fillna(0).astype('int32')\n",
    "mun_merged_df['sum_of_capacity'] = mun_merged_df['sum_of_capacity'].fillna(0).astype('int32')\n",
    "mun_merged_df['carried_capacity_within'] = mun_merged_df['carried_capacity_within'].fillna(0).astype('int32')\n",
    "mun_merged_df['carried_capacity_without'] = mun_merged_df['carried_capacity_without'].fillna(0).astype('int32')\n",
    "mun_merged_df['capacity_left'] = mun_merged_df['capacity_left'].fillna(0).astype('int32')\n",
    "mun_merged_df['service_load'] = mun_merged_df['service_load'].fillna(0).astype('int32')\n",
    "mun_merged_df['mean_load'] = mun_merged_df['mean_load'].fillna(0)\n",
    "\n",
    "mun_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mun_merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение функции для линейной нормализации\n",
    "def min_max_normalization(column):\n",
    "    return (column - column.min()) / (column.max() - column.min())\n",
    "\n",
    "# Определение функции для обратной нормализации\n",
    "def inverse_min_max_normalization(column):\n",
    "    return 1 - (column - column.min()) / (column.max() - column.min())\n",
    "\n",
    "# Определяем суммарный уровень загруженности скейт-парков\n",
    "mun_merged_df['skateparks_load'] = mun_merged_df['skateparks_count'] * mun_merged_df['mean_load']\n",
    "\n",
    "# Применение функции к нужным столбцам\n",
    "normalized_layer = mun_merged_df.copy()  # Создаем копию DataFrame для нормализации\n",
    "normalized_layer[['norm_mean_provision', 'norm_skateparks_load_level']] = normalized_layer[['mean_prov', 'skateparks_load']].apply(min_max_normalization)\n",
    "normalized_layer[['norm_mean_dist']] = normalized_layer[['mean_dist']].apply(inverse_min_max_normalization)\n",
    "\n",
    "# Расчет итоговой оценки\n",
    "normalized_layer['final_assessment'] = normalized_layer['norm_mean_dist'] + normalized_layer['norm_mean_provision'] + normalized_layer['norm_skateparks_load_level']\n",
    "\n",
    "# Округление полученных значений\n",
    "normalized_layer[['norm_mean_provision', 'norm_skateparks_load_level', 'norm_mean_dist', 'final_assessment']] = normalized_layer[['norm_mean_provision', 'norm_skateparks_load_level', 'norm_mean_dist', 'final_assessment']].round(3)\n",
    "\n",
    "# Визуализация данных\n",
    "normalized_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем агрегированные данные с геометрией административных районов и сохраняем в файл\n",
    "\n",
    "mun_merged_gdf = munitipal_blocks.merge(normalized_layer, on='Munitipal')\n",
    "mun_merged_gdf.to_file(os.path.join(data_path,\"Основной сценарий 30 минут - Муниципальные образования.geojson\"))\n",
    "# mun_merged_gdf.to_file(os.path.join(data_path,\"Экспериментальный сценарий 30 минут - Муниципальные образования.geojson\"))\n",
    "\n",
    "# Объединяем агрегированные данные с геометрией административных районов и сохраняем в файл\n",
    "\n",
    "map = mun_merged_gdf.explore(column='final_assessment', cmap='RdYlGn', tiles='CartoDB positron')\n",
    "map.save('Основной сценарий 30 минут - Муниципальные образования.html')\n",
    "# map.save('Экспериментальный сценарий 30 минут - Муниципальные образования.html')\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Работа со слоем зданий в рамках одного района\n",
    "# Выбираем из датафрейма с обеспеченностью ЗДАНИЙ только один район и создаем новый датафрейм\n",
    "\n",
    "adm_name = 'Приморский'\n",
    "mun_name = 'муниципальный округ № 65'\n",
    "\n",
    "build_prov = gpd.read_file(os.path.join(data_path, 'build_prov_30.geojson'))\n",
    "district_build_prov = build_prov[build_prov['Administrative'] == adm_name]\n",
    "#district_build_prov = build_prov[build_prov['Munitipal'] == mun_name]\n",
    "\n",
    "# Вычисление суммарного спроса\n",
    "total_demand = district_build_prov['demand'].sum()\n",
    "print(f\"Суммарный спрос: {total_demand}\")\n",
    "\n",
    "# Вычисление суммарного спроса, удовлетворенного в границах доступности\n",
    "supplyed_demands_within = district_build_prov['supplyed_demands_within'].sum()\n",
    "print(f\"Суммарный спрос, удовлетворенный в границах доступности: {supplyed_demands_within}\")\n",
    "\n",
    "# Вычисление суммарного спроса, удовлетворенного вне границ доступности\n",
    "supplyed_demands_without = district_build_prov['supplyed_demands_without'].sum()\n",
    "print(f\"Суммарный спрос, удовлетворенный вне границ доступности: {supplyed_demands_without}\")\n",
    "\n",
    "# Вычисление суммарного спроса, оставшегося неудовлетворенным\n",
    "total_left_demand = district_build_prov['demand_left'].sum()\n",
    "print(f\"Суммарный спрос, оставшийся неудовлетворенным: {total_left_demand}\")\n",
    "\n",
    "# Вычисление уровня удовлетворения спроса\n",
    "level_demand = round(((district_build_prov['demand'].sum() - district_build_prov['demand_left'].sum()) / district_build_prov['demand'].sum()),3)\n",
    "print(f\"Уровень удовлетворения спроса: {level_demand}\")\n",
    "\n",
    "# Вычисление среднего расстояния до скейт-парка\n",
    "mean_dist = round((district_build_prov['avg_dist'].mean()),2)\n",
    "print(f\"Среднее значение расстояния до скейт-парка: {mean_dist}\")\n",
    "\n",
    "# Вычисление среднего значения обеспеченности\n",
    "mean_prov = round((district_build_prov['provison_value'].mean()),3)\n",
    "print(f\"Среднее значение обеспеченности скейт-парками: {mean_prov}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Работа со слоем сервисов \n",
    "# Выбираем из датафрейма с обеспеченностью СКЕЙТ-ПАРКОВ только один район и создаем новый датафрейм\n",
    "\n",
    "adm_name = 'Приморский'\n",
    "mun_name = 'муниципальный округ № 65'\n",
    "\n",
    "services_prov = gpd.read_file(os.path.join(data_path, 'services_prov_30.geojson'))\n",
    "district_skateparks_prov = services_prov[services_prov['Administrative'] == adm_name]\n",
    "#district_skateparks_prov = services_prov[services_prov['Munitipal'] == mun_name]\n",
    "\n",
    "# Вычисление общего количества скейт-парков\n",
    "skateparks_count = len(district_skateparks_prov)\n",
    "print(\"Общее количество скейт-парков:\", skateparks_count)\n",
    "\n",
    "# Вычисление суммарной вместимости скейт-парков\n",
    "sum_of_capacity = district_skateparks_prov['capacity'].sum()\n",
    "print(f\"Суммарная вместимость скейт-парков: {sum_of_capacity}\")\n",
    "\n",
    "# Вычисление суммарной вместимости, используемой в границах доступности\n",
    "carried_capacity_within = district_skateparks_prov['carried_capacity_within'].sum()\n",
    "print(f\"Суммарная вместимость, используемая в границах доступности: {carried_capacity_within}\")\n",
    "\n",
    "# Вычисление суммарной вместимости, используемой вне границ доступности\n",
    "carried_capacity_without = district_skateparks_prov['carried_capacity_without'].sum()\n",
    "print(f\"Суммарный вместимость, используемая вне границ доступности: {carried_capacity_without}\")\n",
    "\n",
    "# Вычисление суммарной вестимости оставшейся неиспользуемой\n",
    "capacity_left = district_skateparks_prov['capacity_left'].sum()\n",
    "print(f\"Суммарная вестимость, оставшаяся неиспользуемой: {capacity_left}\")\n",
    "\n",
    "# Вычисление среднего количества посетителей скейт-парка\n",
    "service_load = round((district_skateparks_prov['service_load'].mean()),0)\n",
    "print(f\"Среднее количество посетителей скейт-парка: {service_load}\")\n",
    "\n",
    "# Вычисление среднего значения загруженности скейт-парков\n",
    "mean_load = round((district_skateparks_prov['load'].mean()),3)\n",
    "print(f\"Среднее значение загруженности скейт-парков: {mean_load}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iduedu import get_boundary\n",
    "\n",
    "bounds = get_boundary(osm_id=1115367)\n",
    "\n",
    "# Создание GeoDataFrame\n",
    "District = gpd.GeoDataFrame(geometry=[bounds], crs=\"EPSG:4326\")\n",
    "\n",
    "District['District'] = 'Приморский'\n",
    "District['total_demand'] = district_build_prov['demand'].sum()\n",
    "District['total_supplyed_demands_within'] = district_build_prov['supplyed_demands_within'].sum()\n",
    "District['total_supplyed_demands_without'] = district_build_prov['supplyed_demands_without'].sum()\n",
    "District['total_demand_left'] = district_build_prov['demand_left'].sum()\n",
    "District['level_of_demand'] = round(((district_build_prov['demand'].sum() - district_build_prov['demand_left'].sum()) / district_build_prov['demand'].sum()),3)\n",
    "District['mean_dist'] = round((district_build_prov['avg_dist'].mean()),2)\n",
    "District['mean_provision'] = round((district_build_prov['provison_value'].mean()),3)\n",
    "\n",
    "District['skateparks_count'] = len(district_skateparks_prov)\n",
    "District['total_sum_of_capacity'] = district_skateparks_prov['capacity'].sum()\n",
    "District['total_carried_capacity_within'] = district_skateparks_prov['carried_capacity_within'].sum()\n",
    "District['total_carried_capacity_without'] = district_skateparks_prov['carried_capacity_without'].sum()\n",
    "District['total_capacity_left'] = district_skateparks_prov['capacity_left'].sum()\n",
    "District['average_service_load'] = round((district_skateparks_prov['service_load'].mean()),0)\n",
    "District['mean_load_level'] = round((district_skateparks_prov['load'].mean()),3)\n",
    "\n",
    "District.explore(column='mean_provision', cmap='RdYlGn', tiles='CartoDB positron')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
